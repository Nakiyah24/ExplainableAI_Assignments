{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86aba65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install grad-cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3b4f75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bf8dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mango_pth = \"mango_leaf_dataset/\"\n",
    "# mango_dir = os.listdir(mango_pth)\n",
    "# print(len(mango_dir))\n",
    "# print(mango_dir)\n",
    "# diseased = []\n",
    "# healthy = []\n",
    "# for val in range(1, 6):\n",
    "#     print(val)\n",
    "#     diseased.append(\n",
    "#         \"https://raw.githubusercontent.com/Nakiyah24/ExplainableAI_Assignments/main/Assignment5/mango_leaf_dataset/disease_000%s\"\n",
    "#         % val\n",
    "#     )\n",
    "#     healthy.append(\n",
    "#         \"https://raw.githubusercontent.com/Nakiyah24/ExplainableAI_Assignments/main/Assignment5/mango_leaf_dataset/healthy_000%s\"\n",
    "#         % val\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15709059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://raw.githubusercontent.com/Nakiyah24/ExplainableAI_Assignments/main/Assignment5/mango_leaf_dataset/disease_0001.JPG', 'https://raw.githubusercontent.com/Nakiyah24/ExplainableAI_Assignments/main/Assignment5/mango_leaf_dataset/disease_0002.JPG', 'https://raw.githubusercontent.com/Nakiyah24/ExplainableAI_Assignments/main/Assignment5/mango_leaf_dataset/disease_0003.JPG']\n",
      "['https://raw.githubusercontent.com/Nakiyah24/ExplainableAI_Assignments/main/Assignment5/mango_leaf_dataset/healthy_0001.JPG', 'https://raw.githubusercontent.com/Nakiyah24/ExplainableAI_Assignments/main/Assignment5/mango_leaf_dataset/healthy_0002.JPG', 'https://raw.githubusercontent.com/Nakiyah24/ExplainableAI_Assignments/main/Assignment5/mango_leaf_dataset/healthy_0003.JPG']\n"
     ]
    }
   ],
   "source": [
    "raw_github_path = f\"https://raw.githubusercontent.com/Nakiyah24/ExplainableAI_Assignments/main/Assignment5/mango_leaf_dataset\"\n",
    "\n",
    "diseased_lst, healthy_lst = [], []\n",
    "for i in range(1, 4):  # 0001..0005\n",
    "    diseased_lst.append(f\"{raw_github_path}/disease_{i:04d}.JPG\")\n",
    "    healthy_lst.append(f\"{raw_github_path}/healthy_{i:04d}.JPG\")\n",
    "\n",
    "print(diseased_lst)\n",
    "print(healthy_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97a7b67",
   "metadata": {},
   "source": [
    "Step 2:\n",
    "\n",
    "Since I am calling the images from my github repo, the code below is just a simple sanity check to ensure I am able to load the images\n",
    "(Code snippet take from Chatgpt-5 on 11/03/2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a830a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: both URLs load\n"
     ]
    }
   ],
   "source": [
    "def get_img(url):\n",
    "    r = requests.get(url, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    return Image.open(BytesIO(r.content)).convert(\"RGB\")\n",
    "\n",
    "\n",
    "# try first diseased + healthy\n",
    "t1 = get_img(diseased_lst[0])\n",
    "t2 = get_img(healthy_lst[0])\n",
    "print(\"OK: both URLs load\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad90b6e",
   "metadata": {},
   "source": [
    "Step 3: Preprocessing image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f64721",
   "metadata": {},
   "source": [
    "Now that the images are loaded, I need to preprocess and resize the image to 224×224 to match ResNet-50’s input. I will then convert the PIL image to a PyTorch tensor in [0,1], and normalize with the standard ImageNet mean/std so it matches what the pretrained model expects. I will also keep a float RGB copy for the Grad-CAM overlay, and finally add a batch dimension so the input is [1,3,224,224] for the forward pass.\n",
    "\n",
    "\n",
    "Preprocessing code taken from Chatgpt-5 on 11/04/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d09644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IM_SIZE = 224\n",
    "tfm = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IM_SIZE, IM_SIZE)),\n",
    "        transforms.ToTensor(),  # 2) PIL -> tensor in [0,1], shape [C,H,W]\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],  # 3) ImageNet normalization (per channel)\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b42d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(img):\n",
    "    \"\"\"\n",
    "    img: PIL.Image\n",
    "    returns:\n",
    "      rgb -> numpy array, shape [224,224,3], values in [0,1], used only for overlay\n",
    "      x   -> torch tensor, shape [1,3,224,224], normalized, ready for the model\n",
    "    \"\"\"\n",
    "    rgb = np.array(img.resize((IM_SIZE, IM_SIZE)), dtype=np.float32) / 255.0\n",
    "    x = tfm(img).unsqueeze(0)  # add batch dim -> [1,3,224,224]\n",
    "    return rgb, x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
